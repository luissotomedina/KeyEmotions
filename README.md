# KeyEmotions  

**KeyEmotions** is a music generation system that combines emotional expression with machine learning. It uses a Transformer-based model to generate music compositions based on user-selected emotional quadrants. The project processes MIDI data for training and produces emotionally expressive musical outputs.  

This project was developed as the **final thesis for a Master's degree in Artificial Intelligence**, fulfilling academic requirements for the degree.  

---

## ğŸ¹ Features  
- **Emotion-based Music Generation** â€“ Generate music by selecting emotions (e.g., Angry, Excited, Sad, Calm).  
- **Interactive UI** â€“ User-friendly PyQt6 interface for model interaction.  
- **MIDI Processing** â€“ Uses MIDI files for training and generation.  
- **Transformer Model** â€“ Autoregressive architecture for music generation.  

---

## ğŸ¥ Demo  
Check out the demo here:  
ğŸ”— [https://luissotomedina.github.io/portfolio/KeyEmotions/](https://luissotomedina.github.io/portfolio/KeyEmotions/)  

---

## âš™ï¸ Installation  

### ğŸ“‹ Prerequisites  
- **Python 3.12+**  
- **Dependencies**: PyQt6, PyTorch (CUDA optional), Mido, Fluidsynth, Pandas, Matplotlib, etc.  

Install dependencies with:  
```bash
pip install -r requirements.txt
```

## ğŸš€ Usage

### ğŸ”„ Preprocessing
Place MIDI files in `data/raw/`.

Run preprocessing:

```bash
python src/preprocessing/run_preprocessing.py
```

### ğŸ—ï¸ Dataset Creation
To generate the training dataset:

```bash
python src/preprocessing/loader.py
```

### ğŸ“ Training
Configure training parameters in `src/config/default.yaml`, then run:

```bash
python src/train.py
```

### ğŸµ Generating Music
Place `config.json` and `weights.pt` in the same directory.

Launch the UI and specify model paths + output location:

```bash
python src/KeyEmotionsUI.py
```


